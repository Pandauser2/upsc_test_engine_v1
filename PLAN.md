# Feature Implementation Plan

**Overall Progress:** `100%`

## TLDR

Faculty-facing web app: upload UPSC coaching notes (PDF or paste text) â†’ generate 50 Prelims-style MCQs with answer, explanation, difficulty, and topic. Faculty can review/edit, add questions manually if generation yields &lt;50 (partial), and export to .docx. Tech: Next.js (App Router, TypeScript), FastAPI (Python 3.11), PostgreSQL, BackgroundTasks (no Redis for MVP), abstracted LLM layer.

**Current scope:** This plan runs through **Step 9** (backend + APIs complete). **Step 10** (frontend minimal slice) and **Step 11** (Docker and runbook) are planned for later.

## Critical Decisions

- **Exactly 50 MCQs per test** â€” Generate in batches, dedupe (Jaccard/stem), rank (validation heuristic, prefer medium, topic diversity), select best 50; if &lt;50 after max retries â†’ status = partial, notify in-app, allow manual fill.
- **Prompt versioning + cost** â€” Each test stores prompt_version, model, estimated_input/output_tokens, estimated_cost_usd; max_generation_time_seconds = 300; status partial | failed_timeout when applicable.
- **Topic slug enforcement** â€” topic_list table; inject exact slugs into prompt, require verbatim output; post-parse default/drop unknown to avoid FK errors.
- **Auth scope** â€” MVP = faculty only; all documents and tests scoped by user_id; role column (faculty | admin | super_admin) for future scaling.
- **No Redis for MVP** â€” FastAPI BackgroundTasks; add RQ only if â‰¥10 concurrent or generation &gt;15s.

## Tasks

- [x] ğŸŸ© **Step 1: Project bootstrap and infra**
  - [x] ğŸŸ© Create repo folder structure (frontend/, backend/, docker-compose, .env.example) per EXPLORATION Â§2.
  - [x] ğŸŸ© Backend: FastAPI app, config (env), database.py (SQLAlchemy + Postgres), no auth yet.
  - [x] ğŸŸ© Frontend: Next.js App Router + TypeScript, minimal layout and api client base URL.
  - [x] ğŸŸ© docker-compose: Postgres only; backend runnable locally or in container.

- [x] ğŸŸ© **Step 2: Database schema and topic seed**
  - [x] ğŸŸ© Migrations or SQL: users (with role), documents, topic_list, generated_tests, questions per EXPLORATION Â§3.
  - [x] ğŸŸ© Seed topic_list with initial slugs/names (e.g. polity, economy, history, geography, science, environment).
  - [x] ğŸŸ© SQLAlchemy models: User, Document, TopicList, GeneratedTest, Question.

- [x] ğŸŸ© **Step 3: Auth (faculty-scoped)**
  - [x] ğŸŸ© Register (email + password hash), login (JWT), GET /auth/me with role; default role = faculty.
  - [x] ğŸŸ© All document and test APIs filter by current user id (faculty sees only own).
  - [x] ğŸŸ© Dependency: get current user from Bearer token; 401/403 as needed.

- [x] ğŸŸ© **Step 4: Documents API**
  - [x] ğŸŸ© POST /documents/upload (multipart PDF) â†’ save file, create document row (status uploaded), enqueue BackgroundTasks for extraction.
  - [x] ğŸŸ© POST /documents (title, content) â†’ create document (source_type pasted_text, extracted_text = content, status ready).
  - [x] ğŸŸ© GET /documents, GET /documents/{id} (scoped by user_id); PDF extraction service (text-based only) â†’ update document status and extracted_text.

- [ ] ğŸŸ¥ **Step 5: Topics API and prompt slug injection**
  - [ ] ğŸŸ¥ GET /topics â†’ list topic_list (id, slug, name).
  - [ ] ğŸŸ¥ Prompt helper: load topic slugs from DB/config and inject exact list into MCQ-generation prompt; require â€œtopic_tag must be exactly one of: â€¦â€ verbatim.

- [ ] ğŸŸ¥ **Step 6: LLM abstraction and one provider**
  - [ ] ğŸŸ¥ Define LLM interface: generate_mcqs(chunk, topic_slugs) â†’ List[MCQ], validate_mcq(mcq) â†’ str.
  - [ ] ğŸŸ¥ One implementation (e.g. OpenAI); config: LLM_PROVIDER, API key, model, PROMPT_VERSION, max_generation_time_seconds = 300.
  - [ ] ğŸŸ¥ Return structured MCQ with topic_tag as slug; post-parse map unknown slug to default or drop and log.

- [ ] ğŸŸ¥ **Step 7: Pipeline services (chunk, dedupe, rank, validate)**
  - [ ] ğŸŸ¥ Chunking: fixed-size split of extracted_text (character or token estimate).
  - [ ] ğŸŸ¥ Dedupe: Jaccard on stem word sets or n-gram overlap; stem word overlap; same correct_option + overlapping options; keep one per cluster.
  - [ ] ğŸŸ¥ Rank: validation heuristic (prefer no â€œincorrect keyâ€ in critique); prefer medium difficulty; optional topic diversity when selecting top 50.
  - [ ] ğŸŸ¥ Validation: call validate_mcq for each selected MCQ; store critique in validation_result.

- [x] ğŸŸ© **Step 8: Generation job (BackgroundTasks)**
  - [x] ğŸŸ© Single background task: given test_id, document_id, user_id, load test (pending â†’ generating), run chunk â†’ generate batches â†’ dedupe â†’ validate â†’ rank â†’ select best 50 (or fewer) â†’ persist questions; set status completed | partial | failed | failed_timeout; enforce 300s timeout.
  - [x] ğŸŸ© Track and persist estimated_input_tokens, estimated_output_tokens, estimated_cost_usd on test.
  - [x] ğŸŸ© If <50 valid after max retries: set status = partial; in-app visibility (no email required for MVP).

- [x] ğŸŸ© **Step 9: Tests API**
  - [x] ğŸŸ© POST /tests/generate (document_id) â†’ create GeneratedTest row (pending), enqueue job, return test_id.
  - [x] ğŸŸ© GET /tests, GET /tests/{id} (with questions); PATCH /tests/{id}; PATCH /tests/{id}/questions/{qid}; POST /tests/{id}/questions (manual fill); all scoped by user_id.
  - [x] ğŸŸ© POST /tests/{id}/export â†’ .docx three sections (questions, answer key, explanations); simple clean format.

---

## Planned for later

- **Step 10: Frontend minimal vertical slice** â€” Login/register, dashboard, documents (upload/paste/list), tests (list, detail, review/edit, manual add, export), topics dropdown.
- **Step 11: Docker and runbook** â€” docker-compose (Postgres + backend, optional frontend), README with run instructions and env vars.
